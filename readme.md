# Decodable Words Generator

## Description

A Python library for generating a DataFrame with linguistic features of words to assess their decodability. Its output is designed to be used with the [`decodable_words_api`](https://github.com/cpleasants/decodable-words-api) I created, but could be used more widely.

### Features

#### Simple features

- `word` (string): the word itself
- `rank` (number): the rank of the word across the corpus (see [data sources](#data-sources) below)
- `part_of_speech` (string): part of speech of the word (e.g. "noun", "verb")

#### Word Types/Patterns

- `is_vc` (boolean): is a vowel-consonant word (including blends/digraphs), e.g. "is", "ick"
- `is_cvc` (boolean): is a consonant-vowel-consonant word (including blends/digraphs), e.g. "cat", "back", "black"
- `is_cvce` (boolean): is a consonant-vowel-consonant-silent E word (including blends/digraphs). Vowels must be long and C and G must be soft e.g. "like", "page", "price"
- `is_cvcvc` (boolean): is a consonant-vowel-consonant-vowel-consonant word (including blends/digraphs, and can have two consonants in the middle), e.g. "napkin", "within", "finish"
- `has_silent_e` (boolean): contains a silent E at the end.

#### 
`letter_parts`, `sound_parts`, and `indicators` are generated by walking through the CMU dictionary's pronunciation and matching sounds and combinations of sounds with the letters/blends/digraphs/vowel teams in [`phonemes.py`](./decodable_words_generator/phonemes.py). Basically, the code goes through the words and tries to match letters/letter combos with the phonemes. I will use the word "place" as the example below:

- `letter_parts`: list of letters/blends/digraphs/vowel teams that make up the word (e.g. `[pl-, a, c, e]`)
- `sound_parts`: list of sound tuples from the CMU dictionary (e.g. `[(P, L), (EY,), (S,), ]`) -- note that when a letter/digraph/blend is represented by two sounds, it will be a tuple of multiple sounds; also a silent E will be represented by an empty tuple.
- `indicators`: list of `Indicator (Enum)`s that match with the `letter_parts` and `sound_parts`, for easier reference (largely to determine the [word types/patters](#word-typespatterns) above). (e.g. `[Indicator.LETTER_COMBO, Indicator.LONG_VOWEL, Indicator.SOFT_CONSONANT, Indicator.SILENT_E]`) The possible `Indicator` values are:

- `SHORT_VOWEL`
- `LONG_VOWEL`
- `HARD_CONSONANT`
- `SOFT_CONSONANT`
- `LETTER_COMBO` (digraph/blend/vowel team)
- `SILENT_E`
- `UNDECODABLE` (when a sound can't be matched with any of the phonemes, the rest of the letters in the word are labeled as undecodable)


#### Letter Combination Bitmaps

There is a bitmap representing each of the sets of phonemes in [`phonemes.py`](./decodable_words_generator/phonemes.py). There will be a 1. It is in bitmap format for efficiency and for being able to compare the word to a set of criteria (see [`decodable_words_api`](https://github.com/cpleasants/decodable-words-api)). For instance, the word "page" would have the following values:

- `hard_consonants`: `[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]` (1 in the 12th position because 'p' is the 12th in the list of hard consonants)
- `soft_consonants`: `[0, 1]` (1 in the 2nd position because 'g' is the second soft consonant)
- `short_vowels`: `[0, 0, 0, 0, 0]` (because no short vowels are used)
- `long_vowels`: `[1, 0, 0, 0, 0]` (because 'a' is the first long vowel)

Note that, if a word has a listed blend/digraph/vowel team, the containing letters will NOT be reflected in lists of consonants or vowels. For instance, the word `place` will have all 0s in the `hard_consonants` bitmap because `pl` is reflected as a blend in `prefix_blends`. This behavior may change at a later date.

#### Other

- `decodable` (boolean): whether or not the word can be created with a combination of the phonemes in [`phonemes.py`](./decodable_words_generator/phonemes.py). Note there are some that may be surprising; for instance "dog" is not listed as decodable because the CMU dictionary indicates the the 'o' in 'dog' is pronounced like the 'o' in 'abort'. Changes are coming on how something is determined to be decodable, since there are many levels of decodability.

### Data Sources

The data sources I used are:

1. Pronunciations are derived from the CMU Pronouncing Dictionary, which you can check out [here](http://www.speech.cs.cmu.edu/cgi-bin/cmudict). It is downloaded in this project using `nltk`.

2. The corpus I used to identify which words to include in the output, and what their "rank" would be, were put together for the [BabyLM Challenge](https://babylm.github.io/). I utilized three of the cleaned text sources: the [CHILDES](https://childes.talkbank.org/) child-directed speech dataset, the data from the [Childrens Book Test](https://paperswithcode.com/dataset/cbt), and the [Childrens Stories Text Corpus](https://www.kaggle.com/datasets/edenbd/children-stories-text-corpus/data). The data are included in gzip format in the `data/` directory.

3. To determine what blends, digraphs, vowel teams, etc. I should include, I used various scope and sequence documents such as those found [here](https://www.readingrockets.org/sites/default/files/2023-10/NJTSS%20Phonics%20Scope%20and%20Sequence.pdf), [here](https://www.readingrockets.org/sites/default/files/2023-10/Keys%20to%20Literacy%20Systematic-Phonics-Scope-and-Sequence.pdf), and [here](https://www.readingrockets.org/sites/default/files/2023-10/UFLI-Scope-Sequence-phonics.pdf)

4. Parts of speech are determined by the top `pos_` value in the `en_core_web_sm`'s database.

## Installation

The output of this (which is run by [`generate_data.py`](./decodable_words_generator/generate_data.py)) can be found in [here](./decodable_words_generator/processed/10000-words.pickle), which is a pickle of a dataframe that has all the features of words to be used.

You can install the `decodable_words_generator` via the latest wheel:

```bash
pip install https://github.com/cpleasants/decodable-words-generator/releases/download/v0.2.1/decodable_words_generator-0.2.1-py3-none-any.whl
```

This will install the package along with its dependencies. The required dependencies are:

- nltk
- wordfreq
- spacy (with en_core_web_sm)

Make sure you have Python version 3.6 or higher.

The wheel should will also include the output, so it will be usable if you import the package for another project.

## Usage

You can change the constants in `constants.py` to what you would like the generator to use, then run `python generate_data.py`.

To use modules of this package, you can import them, for example:

```python
from decodable_words_generator import word

# Example
w = word.Word("decode")
w.features # the features associated with the word "decode"
```


##### TO DOS:

TODO: Sight words: top X not in Y
TODO: # Open/closed syllable types (ends in vowel)
TODO: R-controlled – car
TODO: Consonant-le – little
TODO: secondary vowel pronunciations
TODO: adding middle-word blends like in pumpkin (instead of just prefix and suffix blends)
TODO: Manually add/ensure certain words are included
TODO: Rhyming?
TODO: Output PDF with options
TODO: separating out 'oo's, and 'ea's, 'ow's, and stuff with y
